In der ersten Kurseinheit haben wir uns einen Überblick über verschiedene konkrete Aufgaben eines Betriebssystems verschafft. Jetzt wollen wir auf drei wichtige Bereiche näher eingehen, mit denen Anwendungs- und System- programmierer in Berührung kommen: Hauptspeicherverwaltung, Synchroni- sation und Dateisysteme.
In modernen Betriebssystemen sind Prozesse die elementaren Arbeitseinheiten. Eine der Hauptaufgaben eines Betriebssystems besteht darin, die existierenden Prozesse quasi-parallel auf der CPU ablaufen zu lassen. Dabei sollte es fair zugehen, die Prozesse sollen sich gegenseitig nicht behindern, und jeder Prozess soll möglichst schnell bearbeitet werden.
Damit der Prozesswechsel ohne Schwierigkeiten funktioniert, werden für jeden bereiten oder blockierten Prozess die Inhalte des Befehlszählers und der übrigen Register zum Zeitpunkt der letzten Unterbrechung im Prozesskontroll- block gespeichert; vergleiche Abschnitt 1.4.1. Man muss aber auch sicherstellen, dass die Inhalte der Hauptspeicherbereiche erhalten bleiben, die den existieren- den Prozessen zugewiesen sind. Hier stellt sich die Frage, wie diese Speicher- platzzuweisung überhaupt erfolgt. Darauf wollen wir jetzt eingehen.
In Abschnitt 1.3.6 sind wir davon ausgegangen, dass das Betriebssystem je- dem Prozess einen zusammenhängenden Hauptspeicherbereich zuweist, der als physischer Adressraum bezeichnet wird. Eine physische Adresse zeigt auf eine aktuelle Speicherstelle im Hauptspeicher. Der Prozess selbst braucht die physi- schen Hardwareadressen nicht zu kennen; er kann logische Adressen verwenden. Eine logische Adresse ist ein Verweis auf eine Speicherstelle, die unabhängig vom physischen Hauptspeicher ist, zum Beispiel die relative Position eines Befehls in einem Programmstück. Ein Programm erzeugt eine Menge von logi- schen Adressen, die zusammen den logischen Adressraum des Prozesses bilden. Bevor ein Zugriff auf den Hauptspeicher stattfindet, muss eine logische Adres- se auf eine physische abgebildet werden. Die Abbildung der logischen auf die physischen Adressen ist die Aufgabe des Betriebssystems. Sie wird oft mit der Hilfe einer Hardware, der MMU (Memory Management Unit), die eine logische Adresse von der CPU erhält und eine physische Adresse ausgibt, erledigt.
Jeder Prozess soll ein Stück vom Hauptspeicher zugewiesen bekommen. Die einfachste Zuweisungsstrategie ist, dass jeder einen zusammenhängenden Be- reich im Hauptspeicher erhält. Es gibt zwei Möglichkeiten:
• Der Hauptspeicher wird fest in zusammenhängende Bereiche unterschied- licher Größe aufgeteilt, und jeder Prozess bekommt einen davon, dessen Größe er mindestens benötigt. Der vom Prozess nicht benutzte Teil des Bereiches ist die sogenannte interne Fragmentierung.
• Jeder Prozess bekommt einen zusammenhängenden Bereich genau der Größe, die er auch angefordert hat.
In beiden Fällen ist die Abbildung einer logischen Adresse auf die physische sehr einfach. Dafür brauchen wir zwei speziellen CPU-Register:
• Basisregister: Zur Speicherung der Startadresse eines Adressraums.
• Grenzregister: Zur Speicherung der Länge des logischen Adressraums.
Die physische Adresse zu einer logischen Adresse ergibt sich durch Addition der Startadresse. Wir sprechen hier auch von relativer Adressierung (relativer zum Basis-Register). Abbildung 2.1 zeigt ein Beispiel zur Umrechnung der Adressen.
Vor der Addition testet die MMU, ob die logische Adresse kleiner als die im Grenzregister ist. Falls nein, erzeugt die MMU eine Unterbrechung, so wird der Speicherschutz gewährleistet. Durch die beiden Register wird gleichzeitig erreicht, dass die Abbildung auf die physischen Adressen flexibel ist, das heißt, dass Programme relokierbar bleiben. Sie können zur Laufzeit in verschiedene Bereiche des Hauptspeichers geladen werden.
Wenn viele Prozesse gleichzeitig existieren, kann es vorkommen, dass der Hauptspeicher zu klein wird, um alle physischen Adressräume aufzunehmen. In diesem Fall werden einige bereite oder blockierte Prozesse durch ein Verschie- ben ihres Adressraums in den Sekundärspeicher ausgelagert. Die Entscheidung darüber, welchee Prozesse ausgelagert werden, trifft Langzeit-Scheduler.
Bevor ein ausgelagerter Prozess weiter rechnen kann, muss er erst wieder eingelagert werden; der Vorgang des Aus- und Einlagerns wird im Englischen als swapping bezeichnet. Beim Einlagern ist es dank der Relokierbarkeit nicht notwendig, den Prozess an exakt dieselbe Stelle im Hauptspeicher zurückzuschreiben, an der er vorher gestanden hat.
Weil sich die Menge der existierenden Prozesse ständig ändert, entstehen zwischen den physischen Adressräumen der Prozesse im Hauptspeicher zwangsläufig Lücken; dieses Phänomen nennt man externe Fragmentierung. Es kann sein, dass keine der vorhandenen Lücken ausreicht, um den physi- schen Adressraum eines neu erzeugten Prozesses aufzunehmen. Dann müsste ein Prozess ausgelagert werden, obwohl insgesamt noch ausreichend Haupt- speicher frei ist – ein wenig effizientes Vorgehen.
Zur Vermeidung dieses Problems kann man verschiedene Strategien anwenden. Eine Möglichkeit besteht darin, die im Hauptspeicher eingelager- ten Prozesse hin und wieder zusammenzuschieben, damit aus vielen kleinen Lücken eine große wird. Eine solche Kompaktifizierung ist aber mit einem ho- hen Aufwand verbunden.
Man gewinnt mehr Flexibilität in der Speichervergabe, wenn man die For- derung aufgibt, dass jeder Prozess ein zusammenhängendes Stück vom Haupt- speicher bekommen soll. Stattdessen kann man zum Beispiel jedem Prozess mehrere zusammenhängende Segmente im Hauptspeicher zuweisen, die unter- schiedlich lang sein dürfen. Dieser im Englischen als segmentation bezeichnete Ansatz ist auch aus der Sicht des Programmierers sinnvoll: Man kann ver- schiedene Programmodule in unterschiedlichen Segmenten unterbringen und zum Beispiel ein weiteres Segment für die Daten verwenden. Die Adressierung innerhalb eines Segments erfolgt analog zu Abbildung 2.1, zur Bezeichnung des Segments wird zusätzlich eine Segmentnummer angegeben.
Ganz beseitigt wird das Problem der externen Fragmentierung, wenn man den Hauptspeicher in sehr viele kleine Stücke gleicher Größe aufteilt und jedem Prozess die erforderliche Anzahl solcher Stücke zuweist; diese Stücke brauchen dabei im Hauptspeicher nicht hintereinander zu liegen. Dieser Ansatz heißt auf Englisch paging. Man teilt den logischen Speicher in gleichgroße Stücke auf, die Seiten (pages) genannt werden. Der physische Speicher wird in Seitenrah- men (frames) aufgeteilt. Eine Seite passt genau in einen Seitenrahmen. Die Seitentabelle (page table) legt fest, welche Seite in welchem Seitenrahmen steht, und liefert damit eine Abbildung vom logischen auf den physischen Speicher; siehe Abbildung 2.2.
Wie funktioniert aber bei Paging die Umrechnung einer logischen Adresse in die physische?
Als Beispiel betrachten wir einen logischen Adressraum der Größe 210 = 1024 Byte mit einer Seitengröße 128 Byte, siehe Abbildung 2.3. Hier zeigt jede logische Adresse auf ein Byte. Also muss es insgesamt 1024 Adressen geben, von 0 bis 1023. Um die 1024 Adressen darzustellen, benötigen wir 10 Bit. Also ist jede Adresse des logischen Adressraums eine binäre Zahl der Länge 10 Bit. Legen wir nun die Seitengröße mit 128 Byte fest, dann hat der logische Adressraum in unserem kleinen Beispiel insgesamt 8 Seiten. Man kann nun die Anzahl der logischen Adressen 1024=8·128 oder 2^10 =2^3 ·2^7 auch als Produkt der Anzahl der Seiten und der Seitengröße schreiben. Daraus ergibt sich die die Idee der für das Paging typischen Aufteilung von logischen Adressen in zwei Abschnitte: Der vordere Teil, hier die ersten 3 Bit, stellt die Seitennummer und der hintere Teil, hier die letzten 7 Bit, den Offset dar.
Rechnen wir nun konkrete Beispiele durch. Wenn wir wissen wollen, in wel- cher Seite z. B. die logische Adresse 254 liegt, dann berechnen wir die Division mit Rest von 254 durch 128 und erhalten den Quotient 1 mit Rest 126, da 254 = 1 · 128 + 126 gilt. Dies bedeutet, dass die logische Adresse 254 in der Seite mit Nummer 1 liegt und innerhalb der Seite die relative Position 126 hat.
Die MMU hat es aber noch einfacher als wir, die lieber im Dezimalsystem rechnen. Sie bekommt die binäre Zahl 0011111110 und interpretiert die ersten drei Bits 001 als die Seitennummer 1 und die letzten 7 Bits 1111110 als den Offset 126, die Division mit Rest ergibt sich mühelos durch die Aufteilung der Bits an der richtigen Stelle. Da eine Seite genau so groß wie ein Seitenrahmen ist, bleibt der Offset einer logischen Adresse gleich dem physischen Offset im Seitenrahmen. Die Konsequenz ist, dass ein Eintrag in der Seitentabelle einfach nur die Nummer des Seitenrahmens zu sein braucht, in dem die Seite auch steht.
Wir fassen die Erkenntnisse aus diesem Beispiel zusammen:
• Die Bits einer logischen Adresse setzen sich aus Seitennummer und Offset zusammen.
• Der Offset ist die Position des Bytes innerhalb einer Seite und eines Seitenrahmens.
• Der Offset in einer logischen Adresse ist derselbe wie in der zugehörigen physischen Adresse.
• Ein Index in die Seitentabelle ist eine Seitennummer und ein Eintrag in der Seitentabelle ist eine Seitenrahmennummer.
• Die Bits einer physischen Adresse werden aus Seitenrahmennummer und Offset zusammengesetzt.
• Für die Umrechnung von logischen in physische Adressen braucht die MMU nur die ersten Bits der Adresse als Seitennummer zu entnehmen und sie als Index in der Seitentabelle zu verwenden. Dort steht die zu- gehörige Seitenrahmennummer, die nur noch mit dem Offset zusammen- gesetzt werden muss, um die physische Adresse zu erhalten.
Wir betrachten als Beispiel, wie die MMU die logische Adresse 254 = 0011111110 auf die physische Adresse abbildet, siehe dazu Abbildung 2.4.
Die MMU erhält die Binärzahl 0011111110, entnimmt die ersten drei Bits 001, sucht in der Seitentabelle an der Stelle 001 nach der Seitenrahmen- nummer und erhält die Zahl 011. Nun setzt die MMU sie mit dem Offset 1111110 zusammen und erhält die physische Adresse 0111111110, das ist umgerechnet 0111111110 = 3 · 128 + 126 = 510.
Das Betriebssystem legt die maximale Größe eines logischen Adressraums fest, unabhängig von einzelnen Prozessen. Es legt auch die Seitengröße fest und damit auch, wie viele Seiten der logische Adressraum eines Prozesses maximal haben kann. Dadurch liegt auch die Anzahl der Bits für die Seitennummer und für den Offset fest. Nun konfiguriert das Betriebssystem die MMU so, dass sie immer die gleiche Anzahl von ersten Bits von einer logischen Adresse als Seitennummer entnimmt und sie als Index für die Suche nach der Seiten- rahmennummer in der Seitentabelle verwendet.
Die Seitentabelle ist Teil des Prozesskontextes. Weil jeder Hauptspeicher- zugriff zunächst einen Zugriff auf die Seitentabelle auslöst, kommt es hier auf Effizienz an. Deshalb werden einige Einträge einer Seitentabelle, die momentan verwendet werden, oft in einem schnellen Speicher auf dem CPU-Chip bzw. in der MMU gehalten.
Durch die Verwendung von Seiten kann zwar keine externe Fragmentierung auftreten; weil aber immer nur ganze Seiten zugeteilt werden, lässt sich eine gewisse interne Fragmentierung nicht vermeiden. Zum Beispiel könnte ein Pro- zess eigentlich nur 3,1 Seiten benötigen, und von den vier zugeteilten Seiten würde die letzte zu 90 % ungenutzt bleiben. Dies stellt aber kein Problem dar bei aktuellen Systemen mit Hunderten von Prozessen und Millionen von Seiten.
Um Prozessen große Hauptspeicherbereiche zuweisen zu können, braucht das Betriebssystem effiziente Mechanismen zur Verwaltung der belegten und freien Seiten, dabei kann auch eine höhere interne Fragmentierung in Kauf genommen werden. Als Beispiel für eine Zuweisungstrategie betrachten wir ein Verfahren in Linux:
Für den physischen Speicher gibt es in Linux einen Seitenallokierer, der Bereiche von aufeinander folgenden Seitenrahmen bereitstellen kann. Dabei bedient er sich der sogenannten Buddy-Strategie: Der Hauptspeicher besteht aus zusammenhängenden Stücken, die jeweils eine Zweierpotenz viele Seiten enthalten. Ein Stück ist entweder belegt oder frei. Wenn der Allokierer einen zusammenhängenden Bereich einer bestimmten Länge benötigt, nimmt er das kleinste freie Stück, das mindestens die erforderliche Länge aufweist. Wenn es mehr als doppelt so lang ist wie der benötigte Bereich, so wird es halbiert; das eine halbe Stück wird benutzt, das andere – sein Buddy4 – bleibt frei. Wann immer ein Stück frei wird und sein Buddy bereits frei ist, werden sie wieder verschmolzen. Dieses Verfahren ist recht einfach zu implementieren, kann aber zu erhöhter interner Fragmentierung führen, denn wenn ein Prozess 33 Seiten benötigt, belegt er schon 64.
Bei Verwendung von mehreren Segmenten oder Seiten je Prozess kann man neben den “privaten” Speicherbereichen, auf die nur der Prozess selbst zugrei- fen darf, auch “öffentliche” Bereiche einrichten, die von mehreren Prozessen gemeinsam benutzt werden können: Es genügt ja, bei jedem Segment oder je- der Seite zu vermerken, welche Prozesse Schreib- oder Leserecht daran haben. So kann zum Beispiel das Code-Segment eines Pascal-Compilers von mehreren Anwendern gleichzeitig benutzt werden, ohne dass jeder Prozess eine eigene Kopie des Compilerprogramms benötigt. Natürlich benötigt jeder Prozess sein eigenes Datensegment und Stacksegment. Außerdem kann man gemeinsame Speicherbereiche für die Kommunikation zwischen Prozessen benutzen; ver- gleiche den Schluss von Abschnitt 1.6. Hierauf gehen wir in Abschnitt 2.3 näher ein.
Am Ende dieses Abschnitts wollen wir die Technik des virtuellen Speichers (virtual memory) zur Hauptspeicherverwaltung besprechen, die heute große Bedeutung erlangt hat und das in praktisch allen modernen Betriebssystemen implementiert ist. Es kombiniert zwei Ansätze, die wir oben vorgestellt haben: die Einteilung des physischen Speichers in Seitenrahmen und die Idee, nur die Informationen im Hauptspeicher zu halten, die gerade benötigt werden.
Für die Programmierer ist die Verwendung virtuellen Speichers sehr ange- nehm: Sie können große logische Adressräume verwenden, ohne auf mögliche physische Grenzen achten zu müssen.
Ein bereiter Prozess wird rechnend gemacht, auch wenn nicht alle seine Seiten in Seitenrahmen des Hauptspeichers stehen; die fehlenden Seiten ste- hen im Sekundärspeicher und sind in der Seitentabelle entsprechend markiert. Das Betriebssystem merkt sich, wo die fehlenden Seiten im Sekundärspeicher stehen. Ein Zugriff auf eine Seite, die nicht im Hauptspeicher steht, wird als Seitenfehler bezeichnet.
Wie kann die MMU wissen, ob eine Seite im Hauptspeicher vorhanden ist, wenn sie in der Seitentabelle nach der Seitenrahmennummer sucht? Dazu wird in der Seitentabelle für jeden Eintrag zusätzlich ein present-Bit verwaltet, das zeigt, ob die Seite im Hauptspeicher vorliegt. Im Fall, dass die Seite nicht im Hauptspeicher vorhanden ist, löst die MMU eine Software-Unterbrechung (trap) aus, da die Software auf eine derzeit nicht verfügbare Seite zugreifen will; vergleiche Abschnitt 1.3.4. Die fehlende Seite wird von der Festplatte gelesen, danach kann der Prozess weiterrechnen. Diese Technik wird als demand paging bezeichnet, bei der eine Seite erst in den Hauptspeicher eingelagert wird, wenn sie auch gebraucht wird.
Es kann beim Einlagern einer Seite vorkommen, dass kein Seitenrahmen mehr frei ist. Dann muss eine andere Seite in den Sekundärspeicher ausgelagert werden. Für die Wahl der auszulagernden Seite gibt es verschiedene Strategien; hier liegt dieselbe Situation vor wie beim Hauptspeicher-Cache, den wir in Abschnitt 1.2.2 betrachtet hatten.
• Die optimale Strategie lagert diejenige Seite aus, die erst am weitesten in der Zukunft wieder benötigt wird, um künftige Seitenfehler möglichst zu vermeiden. Leider steht diese Information in der Regel nicht zur Verfügung. Man muss sich daher mit sub-optimalen Strategien begnügen.
• Die Strategie LRU (least recently used) lagert die Seite aus, deren letz- te Benutzung am weitesten zurückliegt. LRU ist eine Annäherung der optimalen Strategie.
• Es soll möglichst eine Seite ausgelagert werden, die seit der letzten Ein- lagerung nicht mehr verändert wurde. Der Vorteil in diesem Fall ist, dass die Seite nicht mehr auf den Sekundärspeicher zurückgeschrieben zu werden braucht. Dazu wird ein zusätzliches dirty-Bit zu jeder Seite in der Seitentabelle verwaltet, das bei jeder Schreiboperation gesetzt wird. Dadurch kann festgestellt werden, ob die Seite verändert wurde.
Zum Kontext eines Prozesses – das hatten wir in Abschnitt 2.1 gesehen – gehören neben den Registerinhalten auch Informationen über seinen Adress- raum, bei seitenbasierter Speicherorganisation zum Beispiel eine Seitentabelle; vergleiche Abbildung 2.2. Die Verwaltung dieser Informationen kostet beim Prozesswechsel Rechenzeit.
Nun gibt es Probleme, bei deren Lösung man mehrere Prozesse einsetzen möchte, die quasi-parallel ablaufen und dabei alle auf denselben Speicherbe- reich zugreifen. Eine zeitaufwendige Einrichtung neuer Adressräume ist also beim Wechsel zwischen diesen Prozessen nicht erforderlich. Für solche Fälle hat man das Konzept der leichtgewichtigen Prozesse (threads = Fäden) ent- wickelt, das in diesem Abschnitt vorgestellt wird.
Betrachten wir zunächst ein Beispiel. In Rechnernetzen werden oft dedi- zierte Dateiserver (file server) verwendet, Rechner also, die darauf spezialisiert sind, auf Magnetplatten gespeicherte Dateien zu bearbeiten. Von jedem Rech- ner im Netz können Klienten Aufträge an den Server übermitteln, die dieser ausführt und beantwortet.
Weil der Dateiserver nur diese eine Aufgabe wahrnimmt, könnte man mei- nen, dass er im wesentlichen mit einem einzigen Anwendungsprozess aus- kommt. Dieser Prozess schaut in einem Briefkasten (mail box) nach, ob ein Auftrag Ai vorliegt. Wenn das so ist, wird ihm die logische Dateiadresse entnommen, und es wird zunächst geprüft, ob die entsprechenden Daten im Hauptspeicher-Cache des Servers stehen. Ist das nicht der Fall, wird die physi- sche Adresse der Daten auf der Festplatte berechnet. Dann erteilt der Prozess einen Befehl an den Gerätetreiber.
Bei diesem Ansatz gibt es zwei Möglichkeiten: Der Serverprozess könnte jetzt blockieren, bis Gerätetreiber und Controller den Befehl ausgeführt haben. Dann könnte er die Antwort an den Klienten formulieren und abschicken. Dieses Verfahren ist recht einfach, aber sehr ineffizient; denn während der Serverprozess blockiert ist, wäre die CPU des Dateiservers die ganze Zeit über untätig. Oder aber der Serverprozess könnte eine Notiz über den Zustand der Bearbeitung von Auftrag Ai ablegen und schon einmal den nächsten Auftrag aus dem Briefkasten holen. Wenn dann später der Geräte- treiber den zu Ai gehörenden Befehl ausgeführt hat, könnte der Serverprozess mit der Bearbeitung des aktuellen Auftrags Ai+j innehalten und zunächst dem Klienten von Auftrag Ai seine Antwort schicken. Dieses verschachtelte Vorge- hen versucht, mit einem einzelnen Prozess Parallelität nachzumachen; das ist zwar effizient, aber nicht so leicht zu programmieren und sehr unübersichtlich.
Viel eleganter ist die Verwendung mehrerer Serverprozesse, von denen jeder einen kompletten Auftrag sequentiell ausführt. Während einer von ihnen blo- ckiert, weil er auf die Erledigung eines Ein-/Ausgabebefehls wartet, braucht die CPU nicht untätig zu sein, denn inzwischen kann ja ein anderer Serverprozess rechnen.
Diese Serverprozesse greifen alle auf dieselben Hauptspeicherbereiche zu: auf den Briefkasten und den Hauptspeicher-Cache. Es besteht also kein Grund, ihnen individuelle Adressräume zuzuweisen, die den Prozesswechsel verlang- samen. Deshalb verwendet man zur Steuerung eines Dateiservers am besten leichtgewichtige Prozesse (Threads).
Mehrere Threads teilen sich ein Programm, einen Adressraum und dieselben Dateien. Jeder Thread hat aber seine eigenen Registerinhalte — insbesondere seinen eigenen Befehlszähler — und einen eigenen Stapel (Stacksegment).
Solch eine Gruppe von zusammengehörigen leichtgewichtigen Prozessen wird als Task (Aufgabe) bezeichnet.
Leichtgewichtige Prozesse können dieselben Zustände annehmen wie gewöhnliche Prozesse; sie können Kinder generieren und blockierende System- aufrufe ausführen. Beim Zugriff auf den gemeinsamen Speicherbereich kann es zu den Problemen kommen, die wir in Abschnitt 2.3 beschreiben werden, so dass Synchronisationsmechanismen benötigt werden. Dabei kann man allerdings davon ausgehen, dass sich zusammengehörende leichtgewichtige Prozesse kooperativ verhalten.
Es gibt verschiedene Möglichkeiten, leichtgewichtige Prozesse zu imple- mentieren: Kernel-Threads und Benutzer-Threads. Kernel-Threads werden im Betriebssystemkern realisiert, Benutzer-Threads hingegen im privaten Speicherbereich eines Prozesses. Dazu kommen noch Mischformen aus beiden Implementierungen.
Bei einer Implementierung als Kernel-Threads im Betriebssystemkern wer- den die leichtgewichtigen Prozesse genau wie die schwergewichtigen behandelt; insbesondere wird jeder Prozesswechsel und das Scheduling vom Kern ausge- führt, wie in Abschnitt 1.4.1 beschrieben wurde. Diese Situation liegt zum Beispiel in Linux vor. Bei der Generierung eines leichtgewichtigen Prozesses verwendet man nicht den Befehl fork, wie wir ihn in Abschnitt 1.7 kennenge- lernt haben, sondern clone. Hierdurch wird ein Kindprozess erzeugt, der nicht nur sein Programm sondern auch seinen Speicherbereich vom Erzeugerprozess erbt.
Benutzer-Threads hingegen werden mit Hilfe von Bibliotheksprozeduren auf Benutzerebene implementiert. Wann immer ein Thread einen Systemaufruf ausführen möchte — zum Beispiel eine Semaphoroperation —, so ruft er statt- dessen eine Bibliotheksprozedur auf. Sie entscheidet, ob der Thread suspendiert werden muss. Wenn das der Fall ist, vertauscht die Bibliotheksprozedur die ak- tuellen Registerinhalte mit denen eines anderen bereiten Threads, ohne dass der Betriebssystemkern involviert wird; das Betriebssystem weiß also gar nichts von der Existenz der Threads und behandelt den gesamten Task wie einen ein- zigen schwergewichtigen Prozess. So ergeben sich sehr kurze Umschaltzeiten beim Wechsel zwischen den leichtgewichtigen Prozessen. Außerdem lässt sich das Scheduling der Threads bei diesem Ansatz vom Anwendungsprogrammie- rer steuern.
Diesen Vorteilen steht aber ein gravierender Nachteil gegenüber: Wenn ein leichtgewichtiger Prozess eines Tasks einen blockierenden Systemaufruf durch- führt, wird der gesamte Task blockiert. Bei einer Implementierung leichtge- wichtiger Prozesse im Betriebssystemkern könnte dagegen jetzt ein anderer Thread desselben Tasks rechnend gemacht werden.
Aus Abschnitt 2.2 wissen wir, dass die Threads eines Prozesses das gemeinsa- me Datensegment benutzen können, um miteinander zu kommunizieren. Bei- spielsweise können sich Threads dort Nachrichten schreiben und lesen, und so miteinander kommunizieren.
Angenommen, in einem Rechnersystem finden in unregelmäßigen Abstän- den Ereignisse statt, über deren Häufigkeit Buch geführt werden soll. Ein Thread namens Beobachter zählt immer dann eine Variable Zähler hoch, wenn ein Ereignis stattgefunden hat. Ein zweiter Thread namens Berichterstatter druckt hin und wieder den Zählerstand aus und setzt die Variable auf Null zurück. Beide Threads existieren für immer.
Die Variable Zähler liegt in dem gemeinsamen Datensegment der Threads, auf den sie lesend und schreibend zugreifen können; vergleiche Abschnitt 2.1. Sie wird anfangs auf Null gesetzt.
Man sollte meinen, dass zu jedem Zeitpunkt die Summe aller ausgedruckten Zählerstände zusammen mit dem aktuellen Wert von Zähler die Anzahl aller Ereignisse angibt, die bisher stattgefunden haben.
Das stimmt aber nicht! Denn die beiden Threads laufen (quasi-)parallel ab, und dadurch kann es zum Beispiel zu folgender Ausführungsreihenfolge der Anweisungen kommen:
Wenn der Berichterstatter gerade den aktuellen Zählerstand ausgedruckt hat und in diesem Moment wegen Ablauf seiner Zeitscheibe unterbrochen wird, könnte der Beobachter rechnend werden. Angenommen, jetzt treten viele Ereignisse ein, und die Variable Zähler wird entsprechend hochgezählt. Sobald der Berichterstatter wieder rechnend wird, setzt er als erstes den Zähler auf Null, und alle soeben aufgetretenen Ereignisse sind verloren.
Dieser Fehler hängt nicht davon ab, dass wir nur über eine CPU verfügen; er kann ebenso auftreten, wenn die Threads auf verschiedenen Prozessoren mit unbekannten Geschwindigkeiten ablaufen. Hier liegen sogenannte Wettkampf- bedingungen (race conditions) vor; welcher Thread als erster ein bestimmtes Ziel erreicht, ist nicht vorhersehbar.
Die Fehlerursache liegt vielmehr darin, dass die beiden Anweisungen print(Zähler); Zähler := 0 im Programm des Berichterstatters einen kritischen Abschnitt (critical section) bilden, der keine Unterbrechung durch den Beobachter verträgt. Ein kritischer Abschnitt ist ein Abschnitt im Programm, in dem
• gemeinsame Ressource wie z.B. Variable und Datenstrukturen benutzt werden,
• auf die mehrere Threads oder Prozesse lesend und schreibend zugreifen, so dass
• eine race condition entstehen kann.
Offenbar muss man die beiden Threads synchronisieren, um zu verhindern, dass beide zur gleichen Zeit in ihren kritischen Abschnitt eintreten. Also soll die Synchronisation den exklusiven Zugriff auf den kritischen Abschnitt garantieren.
Eine naheliegende Möglichkeit besteht darin, eine Synchronisationsvariable namens switch zu verwenden, die wie ein Schalter den Zugang zu den kriti- schen Abschnitten regelt. Hat sie den Wert 0, so ist der Beobachter an der Reihe, beim Wert 1 darf der Berichterstatter rechnen: Dabei steht no-op für no operation; der Thread Beobachter bleibt also in der while-Schleife und tut nichts, bis die Bedingung falsch wird, das heißt, bis switch den Wert 0 erhält. Dann führt er seinen kritischen Abschnitt aus und setzt switch auf 1, damit der Berichterstatter in seinen kritischen Abschnitt eintreten kann.
Man sieht schnell, dass diese Lösung zwei Nachteile aufweist:
• Beide Threads verbrauchen wertvolle CPU-Zeit, während sie in ihren while-Schleifen warten. Ein solches geschäftiges Warten (busy waiting) ist unerwünscht.
• Die beiden Threads können nur abwechselnd in ihre kritischen Abschnitte eintreten. Wenn einer von ihnen innerhalb des kritischen Abschnitts be- endet werden sollte, kann der andere nie wieder den kritischen Abschnitt betreten.
Diese Schwierigkeiten lassen sich vermeiden, wenn man das Konzept des Semaphors verwendet. Es wurde von Dijkstra zur Lösung von Problemen entwickelt, bei denen mehrere Prozesse oder Threads ein Betriebsmittel bele- gen wollen, von dem insgesamt n Stück zur Verfügung stehen. Dabei kann es sich um n freie Speicherplätze handeln, um n CPUs oder um das Recht, in den kritischen Abschnitt eintreten zu dürfen. Im letzten Fall ist n = 1, weil ja zu einem Zeitpunkt immer nur ein Prozess seinen kritischen Abschnitt betreten darf.
Ein Semaphor S kann als abstrakter Datentyp spezifiziert werden. Der Zustand von S besteht aus der Anzahl freier Betriebsmittel, gespeichert in einer Zählvariablen count , und einer Prozessmenge W . Falls count 6= 0, so ist W leer, ansonsten enthält W alle Prozesse, die sich bisher vergeblich um ein Betriebsmittel bemüht haben und darauf warten, dass wieder eines frei wird.
Auf S sind zwei Operationen definiert, down und up. Wenn ein Prozess ein Betriebsmittel benutzen will, ruft er die Operation down auf.
Wenn ein Prozess sein Betriebsmittel wieder freigibt, ruft er die Operation up auf.
Damit lässt sich das Problem vom Beobachter und Berichterstatter folgen- dermaßen lösen. Zunächst wird eine Semaphorvariable S definiert und ihre Zählvariable count auf 1 gesetzt. Die Variable Zähler erhält – wie oben – den Anfangswert 0.
Damit der Semaphor korrekt arbeitet, müssen die Operationen down und up in geeigneter Weise implementiert sein. So darf zum Beispiel bei einem Aufruf down(S) nach dem Test der Zählvariablen count kein anderer Prozess down(S) aufrufen, bevor der Wert von count — falls er positiv war — um eins heruntergezählt worden ist. Operationen, die nicht unterbrochen werden dürfen, nennt man atomare Operationen oder auch unteilbare Operationen; hiervon handelt die folgende Übungsaufgabe.
Wir stellen fest: Die Semaphor-Operationen down und up zur Realisierung
kritischer Abschnitte enthalten selbst kritische Abschnitte! Haben wir also un-
ser Problem nur verlagert? Ja; aber dadurch wird es leichter lösbar. Wenn
man die Semaphor-Operationen nämlich im Betriebssystem implementiert,
kann man während der Ausführung von down oder up alle Unterbrechungen
sperren und damit den gerade beschriebenen Fehler verhindern; vergleiche Abschnitt 1.3.4.
Falls die Semaphorvariable count nur die Werte 0 oder 1 annehmen kann, spricht man von einem binären Semaphor, der meistens dazu verwendet wird, den wechselseitigen Ausschluss (mutual exclusion) von zwei Prozessen sicher- zustellen.
Beim folgenden Erzeuger-Verbraucher-Problem treten Semaphore auf, deren Zählvariablen größere Werte als 1 annehmen können. Angenommen, ein Erzeugerprozess E erzeugt bestimmte Objekte, die von einem Verbraucher- prozess V verbraucht werden. Zum Beispiel kann E ein Compiler sein, der Anweisungen in Assemblersprache erzeugt, und V ein Assembler, der sie ent- gegennimmt und daraus Anweisungen im Binärcode macht. Der Erzeuger über- gibt die Objekte nicht einzeln an den Verbraucher, sondern legt sie in einem Zwischenspeicher ab, einem sogenannten Puffer (buffer). Wann immer der Ver- braucher ein Objekt verbraucht hat, holt er sich aus dem Puffer das nächste. Der Puffer kann maximal n Objekte speichern.
Hierbei treten folgende Synchronisationsprobleme auf:
• Erzeuger und Verbraucher sollten nicht gleichzeitig auf den Puffer zugreifen;
• der Erzeuger sollte nicht versuchen, ein Objekt in den vollen Puffer zu schreiben;
• der Verbraucher sollte nicht versuchen, ein Objekt aus dem leeren Puffer zu entnehmen.
Die Zählvariable count des Semaphors Frei wird in Pascal-Notation mit Frei.count bezeichnet; sie gibt an, wieviele Plätze im Puffer mindestens noch frei sind. Der Zählerstand von Belegt entspricht der Anzahl der mindestens belegten Plätze. Beachten Sie, dass die beiden Prozesse “über Kreuz” sym- metrisch sind. Das Programm löst die obigen drei Synchronisationsprobleme. Bevor ein Erzeuger ein neues Objekt in den Puffer legt, muss er prüfen, dass der Puffer nicht schon voll ist, indem er eine down-Operation auf Frei ausführt. Wenn der Puffer voll ist, muss er sich in die Warteschlange von Frei stellen. Wenn nicht, dann kann der Erzeuger versuchen, auf den Puffer zuzugreifen. Er führt eine down-Operation auf Zugriff aus, damit er warten muss, wenn gerade ein paraleller Zugriff auf den Puffer stattfindet. Nachdem das erzeugte Objekt in den Puffer gelegt wird, führt der Erzeuger eine up-Operation auf Zugriff aus, um den exklusiven Zugriff wieder freizugeben. Danach wird noch eine up-Operation auf Belegt ausgeführt, da jetzt ein Objekt mehr im Puffer liegt. Bevor der Verbraucher auf den Puffer zugreift, muss er zuerst testen, ob der Puffer leer ist. Er führt eine down-Operation auf Belegt aus. Wenn der Puffer leer ist, dann stellt er sich in die Warteschlange von Belegt. Wenn nicht, kann er auf den Puffer zugreifen. Nachdem der Verbraucher den exklusiven Zugriff freigegeben hat, führt er eine up-Operation auf Frei aus, da der Puffer einen freien Platz mehr hat.
Ein berühmtes Problem der Prozess-Synchronisation ist das auf Dijkstra zurückgehende Problem der dinierenden Philosophen. Es lautet wie folgt: n   2 Philosophen sitzen an einem runden Tisch. Jeder dieser n Philoso- phen durchläuft zyklisch die drei Zustände „Denken“, „Hungrig“ und „Essen“. Um essen zu können, braucht jeder Philosoph gleichzeitig ein links und ein rechts von seinem Teller liegendes Essstäbchen. Den n Philosophen stehen aber nur insgesamt n Stäbchen zur Verfügung (zwei an dem runden Tisch aneinander angrenzende Teller sind durch genau ein Stäbchen getrennt, vgl.
Abbildung 2.6). Wenn also zwei benachbarte Philosophen gleichzeitig hungrig werden und dann essen wollen, so wird es Schwierigkeiten geben.
Betrachten wir jeden Philosophen als Prozess, so wird es gerade darauf an- kommen, das gleichzeitige Essen zweier benachbarter Philosophen (oder auch nur den Versuch dazu) zu vermeiden.
Die einfachste Form einer Lösung dieses Problems wäre die Einführung von Semaphoren S0,...,Sn 1 für jedes Stäbchen. Für jeden Philosophen i liefere links(i) (rechts(i)) den Index des Semaphors für das Stäbchen links (rechts) vom ihm. Alle Semaphore werden mit 1 initialisiert.
Man löst das Problem dieser Blockade, indem ein Philosoph nur dann Stäb- chen aufnehmen darf, wenn beide frei sind. Dazu braucht man einen Semaphor mutex, der mit 1 initialisiert wird. Bevor ein Philosoph versucht, die Stäb- chen zu nehmen oder zurückzulegen, führt er eine down-Operation auf mutex aus. Nachdem er die Stäbchen genommen oder zurückgelegt hat, führt er eine up-Operation auf mutex aus. Es kann also immer nur ein Philosoph gleichzei- tig Stäbchen nehmen oder zurücklegen. Kann aber ein Philosoph nicht beide Stäbchen aufnehmen, so wird er schlafen gelegt. Das wird realisiert, indem je- der Philosoph i einen Semaphor p[i] hat, der mit 0 initialisiert wird. Das Ziel des Semaphors p[i] ist, den i-ten hungrigen Philosoph zu blockieren, d. h. war- ten zu lassen, falls eins seiner beiden Stäbchen nicht frei ist. Jeder Philosoph i benötigt noch eine Statusvariable status[i], um zu verfolgen, ob er gerade hungrig ist oder denkt oder isst. Ein Philosoph kann nur in den Zustand von Essen übergehen, wenn seine beiden Nachbarn nicht beim Essen sind. Dieser status[i] wird zu Anfang auf Denken gesetzt.
Die Prozedur teste(i) stellt fest, ob der Philosoph i hungrig ist und die zwei Stäbchen bekommen kann. Wenn ja, dann geht er in den Zustand „Essen“ über. Man beachten, dass die Prozedur teste sowohl von Philosophen für sich selbst aufgerufen wird (in stäbchen_nehmen) als auch für seine Nachbarn (in stäbchen_weglegen). Außerdem wird es hier nicht automatisch „gerecht“ zuge- hen, was die Verteilung der Essenszeiten angeht, wie man sich leicht überlegen kann.
Neben den Prozessen sind Dateien und Dateisysteme die Objekte, mit denen jeder Anwender und Programmierer zu tun hat; sie bestimmen zu einem großen Teil seine Arbeitsumgebung.
Wir wollen uns im folgenden Abschnitt zunächst mit der logischen Sicht auf die Dateien beschäftigen, die ein Dateisystem den Benutzern bietet. Dabei interessieren uns besonders Verzeichnisse und Pfade, wie UNIX sie verwendet, und die Befehle zu ihrer Verwaltung.
In Abschnitt 2.4.2 diskutieren wir dann, wie das Dateisystem intern die logische Sicht der Dateien auf die physische Realität des Sekundärspeichers abbildet.
Dateisysteme dienen der Verwaltung von Dateien (files). Eine Datei ist – wie schon in Abschnitt 1.6 erwähnt – eine Folge von Datensätzen, die zusammengehörige Information enthalten. Für den Benutzer wird eine Datei durch ihren Namen (filename) kenntlich. Es empfiehlt sich, sprechende Namen zu vergeben, aus denen der Inhalt der Dateien ersichtlich wird.
Oft wird der Typ einer Datei durch eine Erweiterung (extension), auch Suffix genannt, des Dateinamens bezeichnet. Wer zum Beispiel mit dem For- matierer LATEX arbeitet, kennt wohl Dateien der Art
SeminarArbeit.tex SeminarArbeit.pdf SeminarArbeit.aux, die – in dieser Reihenfolge – einen zu formatierenden Text namens Seminar- Arbeit, den geräteunabhängig als PDF formatierten Text zum Anzeigen und Drucken und die automatisch generierten Hilfsinformationen zum Formatieren enthalten.
In UNIX sind Dateien in Verzeichnissen (directories) zusammengefasst. Der Benutzer befindet sich zu jedem Zeitpunkt in einem aktuellen Arbeitsverzeichnis, dessen Namen man sich mitt dem Befehl pwd (print working directory) ausgeben lassen kann.
Ein Verzeichnis kann Dateien und weitere Unterverzeichnisse enthalten. Welche Objekte im aktuellen Verzeichnis enthalten sind, kann man sich mit dem Befehl ls auflisten lassen; durch Angabe weiterer Parameter lässt sich festlegen, welche Attribute der im Verzeichnis enthaltenen Objekte ausgegeben werden.
Die nächsten Eintragungen haben mit den Zugriffsberechtigungen auf die Objekte zu tun. Hierzu ein paar Vorbemerkungen. Auf einem Rechner mit mehreren Benutzern kann man nicht jedem Benutzer den Zugriff auf alle über- haupt vorhandenen Daten erlauben; in einem solchen System ließe sich kein wirksamer Datenschutz realisieren, und die Gefahr der versehentlichen Zerstö- rung von Information wäre zu groß. Es ist aber auch nicht sinnvoll, wenn jeder Benutzer nur auf seine eigenen Dateien zugreifen darf, denn das beschränkt die Möglichkeit zur Kooperation. Benötigt werden also Mechanismen zur Ver- gabe abgestufter Zugriffsrechte. Naheliegende Ansätze wären, zu jeder Datei eine Liste aller Benutzer anzulegen, die Zugriff auf die Datei haben, oder für jeden Benutzer eine Liste mit allen Dateien, auf die er zugreifen darf. Beide Verfahren sind recht ineffizient.
In UNIX wird deshalb ein anderer Weg beschritten, um die Zugriffsrechte auf eine Datei oder ein Verzeichnis festzulegen. Man teilt die Systembenutzer in drei Klassen ein: den Eigentümer des Objekts – er wird in diesem Zusam- menhang als user bezeichnet –, die Arbeitsgruppe (group), der er angehört, und alle übrigen Systembenutzer (other). In der Ausgabe des Befehls ls -ls steht in der vierten Spalte von links der Name des Besitzers, in der fünften der Name seiner Arbeitsgruppe.
Außerdem unterscheidet man zwischen den Zugriffsarten Lesen (read), Schreiben (write) und Ausführen (execute). Das Recht auf Schreibzugriff (wri- te permission) erlaubt dabei auch ein Überschreiben und enthält deshalb das Recht zum Löschen des Objekts. Bei einem Verzeichnis bedeutet das Aus- führungsrecht, dass man dieses Verzeichnis zum aktuellen Arbeitsverzeichnis machen darf.
Nun kommen wir zur Ausgabe der Objekte in unserem Beispielverzeichnis zurück und betrachten die zweite Spalte von links. Das erste Zeichen bei den Objekten archiv und privat ist ein d für directory — hier handelt es sich also um Verzeichnisse. Bei den übrigen Objekten finden wir an dieser Stelle einen Strich; dies sind Dateien.
Die in der zweiten Spalte auf das erste Zeichen folgenden 9 Zeichen be- schreiben die Zugriffsberechtigungen für die verschiedenen Benutzerklassen in der Reihenfolge rwxrwxrwx ugo. Wo ein Strich steht, ist das betreffende Recht nicht vergeben. So hat zum Beispiel die vierte Zeile
-rwxr-xr-- 1 mueller bteam 1730 Jul 15 10:53 myprogram folgende Bedeutung: Der Eigentümer Müller hat an seiner Datei myprogram das Lese-, Schreib,- und Ausführungsrecht. Die übrigen Mitglieder seiner Ar- beitsgruppe bteam dürfen das Programm lesen und ausführen, aber nicht schreiben. Alle übrigen Benutzer dürfen das Programm zwar lesen, aber weder ausführen noch schreiben. Und die Zeile drwxrwxr-- 4 mueller bteam 512 Jul 27 16:43 archiv besagt, dass der Eigentümer und alle übrigen Gruppenmitglieder im Verzeich- nis archiv Einträge von Objekten lesen und schreiben dürfen und dieses Ver- zeichnis auch zum aktuellen Arbeitsverzeichnis machen dürfen; hierzu verwen- det man den Befehl cd archiv, wobei cd für change directory steht. Alle übrigen Benutzer dürfen aber nur lesen, welche Objekte im Verzeichnis archiv enthal- ten sind.
Angenommen, Müller möchte allen Mitgliedern seiner Arbeitsgruppe ge- statten, an seinem Programm myprogram mitzuschreiben. Dann kann er den Befehl
chmod g+w myprogram erteilen. Hierbei steht chmod für change mode, und g+w bedeutet, dass der Gruppe das Schreibrecht hinzugefügt wird; mit g-x hätte Müller ihr das Recht zum Ausführen des Programms entzogen. Außer dem Superuser kann nur der Eigentümer eines Objekts dessen Rechte ändern.
Rechte an einem Objekt vererben sich übrigens nicht automatisch auf klei- nere Benutzerklassen: Im Beispiel
----r--r-- 1 meier cteam 402 Jul 27 16:43 akte hat jeder Benutzer das Leserecht an der Datei akte – nur ihr Eigentümer Meier nicht! Meier kann sich jedoch das Leserecht wieder zuweisen.
Wer im aktuellen Verzeichnis schreibberechtigt ist, kann dort mit touch neudatei oder mkdir neuverzeichnis eine neue Datei mit 0 Byte Länge beziehungsweise ein neues Verzeichnis anlegen, hierbei steht mk für make.
Der Befehl rm neudatei löscht die Datei neudatei wieder; natürlich bedeu- tet rm hier remove. Zum Löschen eines leeren Verzeichnisses verwendet man rmdir. Mit dem Befehl rm -r neuverzeichnis kann man das Verzeichnis neuverzeichnis und alle darin enthaltenen Objekte löschen, selbst wenn man an diesen nicht schreibberechtigt ist! (Vorsicht mit dieser Option!). Mit dem Befehl cp altdatei neudatei kann man eine Datei, die man lesen darf, kopieren (copy) und dabei den Namen der Kopie festlegen. Dabei wird man zum Eigentümer der Kopie. Man kann mit Varianten dieses Befehls auch Verzeichnisse kopieren.
Jedem Benutzer wird vom Systemverwalter ein Heimatverzeichnis (home directory) zugewiesen, das bei jeder Anmeldung (log-in) zunächst das aktuelle Arbeitsverzeichnis ist. Für den Benutzer Müller könnte das zum Beispiel das Verzeichnis mueller sein. Mit dem Befehl cd projekt kann er sich nach der Anmeldung in das Verzeichnis projekt begeben.
Wir sehen, dass sowohl Meier als auch Müller eine Datei namens myprogram besitzen. Dies führt aber nicht zu Verwechselungen, denn für das Filesystem sind die Zugriffspfade im Verzeichnisbaum entscheidend — und die sind bei diesen beiden Dateien verschieden! So hat Meiers Datei den vollständigen Na- men
/home/meier/myprogram,
während die vollständige Bezeichnung für Müllers Datei
/home/mueller/projekt/myprogram
lautet. Neben diesen absoluten Pfadnamen können die Benutzer auch relative Pfadnamen verwenden, die im aktuellen Arbeitsverzeichnis beginnen. So kann Müller seine Datei einfach durch myprogram ansprechen, während er die gleichnamige Datei von Benutzer Meier
../../meier/myprogram
nennen kann; dabei bezeichnet “..” stets das Elternverzeichnis. Für das aktuelle Verzeichnis gibt es übrigens die Bezeichnung “.”, und das Heimatverzeichnis wird mit “⇠” bezeichnet.
Diese Pfadnamen werden auch bei zahlreichen Befehlen verwendet. Will zum Beispiel Müller seine Datei entwurf.tex an Meier abgeben, so kann er den Befehl mv für move dazu verwenden und
mv entwurf.tex ../../meier eingeben. Voraussetzung ist, dass Müller im Verzeichnis meier schreibberechtigt ist. Name, Eigentümer und Zugriffsrechte der Datei bleiben hierbei erhalten. Damit Meier mit der Datei entwurf.tex etwas anfangen kann, sollte Müller vorher zumindest allen Benutzern seiner Gruppe Schreib- und Leserecht gewähren; siehe Übungsaufgabe 2.16.
Um zu erreichen, dass Meier und Müller beide bequem an der Datei entwurf.tex arbeiten können, gibt es noch eine andere Möglichkeit: Müller setzt zunächst die Zugriffsrechte um, wie gerade besprochen. Dann gibt er den Befehl
ln entwurf.tex ../../meier
ein. Hierbei steht ln für link und bewirkt, dass im Verzeichnis meier ein weiterer Verweis (hard link) auf die Datei entwurf.tex angelegt wird. Dieser Verweis heißt hier dann ebenfalls entwurf.tex, könnte aber auch einen anderen Namen bekommen. Wenn einer von beiden an der Datei Änderungen vornimmt, sind sie auch für den anderen sichtbar. Wenn Meier oder Müller seinen Eintrag entwurf.tex löscht, bleibt die Datei für den anderen erhalten, da noch ein an- derer Verweis auf diese Datei besteht.
Damit können wir in unserer Beispielausgabe des Befehls ls -ls auch die dritte Spalte von links erklären: Sie gibt die Anzahl der Verweise auf das be- treffende Objekt an. Bei Dateien beträgt diese Zahl eins plus die Anzahl der zusätzlich eingerichteten Verweise; bei entwurf.tex sehen wir deshalb eine 2. Bei Verzeichnissen verweist zusätzlich jedes Verzeichnis auf sich selbst (mit “.”) und auf sein Elternverzeichnis (mit “..”). Deshalb steht bei dem leeren Verzeichnis archiv eine 2 und bei privat die Zahl 4, weil privat noch zwei Unterverzeichnisse enthält, vergleiche Abbildung 2.7.
In Abschnitt 2.4.1 haben wir uns damit beschäftigt, welche logische Sicht auf die Dateien das Dateisystem als der für die Ein- und Ausgabe zuständige Teil des Betriebssystems den Benutzern bietet. Jetzt wollen wir untersuchen, wie das Dateisystem intern die logische Sicht auf die physische Sicht abbildet.
Während eine Datei sich aus logischer Sicht als eine Folge von Datensätzen darstellt, ist sie aus physischer Sicht eine Folge von gleich großen Blöcken. Auf der Magnetplatte wird jeder Block mit Zusatzinformation versehen und in einem Sektor gespeichert; vergleiche Abschnitt 1.2.3 und Übungsaufgabe 2.14.
Aus zwei Gründen wäre es wünschenswert, die Blöcke einer Datei mög- lichst hintereinander auf der Platte zu speichern: Bei sequentiellem Zugriff auf mehrere aufeinander folgende Blöcke wird dadurch die Zeit für die Bewe- gung der Schreib-/Leseköpfe minimiert. Und bei wahlfreiem Zugriff auf einzelne Blöcke kann man leicht die Blocknummern berechnen: Wenn der i te Block der Datei gelesen werden soll und die Datei bei Block b beginnt, so muss der Gerätetreiber einen Leseauftrag für den Block mit der Nummer b + i erhalten.
Leider führt der Wunsch nach zusammenhängender Speicherung von Dateien zu demselben Problem der externen Fragmentierung, das wir in Abschnitt 2.1 bei der Hauptspeichervergabe beobachtet haben: Zwischen den Dateien entstehen Lücken, die sich nicht mehr zur Speicherung längerer Datei- en eignen. Eine Kompaktifizierung durch Zusammenschieben der vorhandenen Dateien ist zwar möglich, wird aber wegen des hohen Zeitaufwands in der Regel nicht bei laufendem Betrieb durchgeführt.
Man hat deshalb auch beim Sekundärspeicher die Forderung nach zusam- menhängender Speicherung aufgegeben und stattdessen Speicherverfahren ent- wickelt, bei denen die Blöcke einzeln gespeichert werden können, wo immer gerade Platz frei ist. Hier stellt sich die Frage, wie man die Blöcke effizient wiederfindet.
Eine naive Lösung könnte darin bestehen, die Blöcke einer Datei in einer Liste zu verketten: In dem Verzeichnis, das die Datei enthält, wäre dann beim Dateinamen die physische Adresse von Block Nr. 0 der Datei aufgeführt; am Schluss dieses Blocks stünde die physische Blocknummer von Dateiblock Nr. 1, und so fort. Diese Idee geht zwar effizient mit dem Speicherplatz um und ver- meidet externe Fragmentierung, sie hat aber einen anderen schwerwiegenden Nachteil: Wahlfreier Zugriff wird nicht unterstützt. Um Block Nr. i zu lesen, sind nämlich i Zugriffe auf den externen Speicher auf alle Blöcke 0 bis i   1 notwendig.
Die Betriebssysteme MS-DOS und OS/2 umgingen diesen Nachteil durch einen einfachen Trick: Verkettet werden nicht die Blöcke, sondern ihre physi- schen Blockadressen. Abbildung 2.8 zeigt ein Beispiel für diese als file-allocation table bekannte Struktur. Sie wird als
FAT : array[0..MaxBlockNr   1] of BlockNr implementiert und enthält für jeden Block der Magnetplatte einen Eintrag.
Im Dateiverzeichnis steht – wie oben – bei jedem Dateinamen myprogram die physische Adresse i von Dateiblock Nr. 0. Der Eintrag FAT[i] enthält dann die Adresse j des zweiten Blocks der Datei myprogram, in FAT[j] steht die Adresse
des dritten Blocks, und so fort. Für den letzten Block l der Datei hat FAT[l] den speziellen Wert eof, der für end of f ile steht. Für freie Blöcke lautet der
Eintrag free. Wenn eine Datei verlängert werden soll, kann man also die FAT dazu benutzen, einen unbelegten Block zu finden.
Die FAT wird auf konsekutiven Blöcken im Externspeicher abgelegt. Die Blockgröße sollte so bemessen sein, dass die FAT zur Laufzeit in den Cache im Hauptspeicher passt. Dann kann man die physische Adresse des i-ten Blocks einer Datei mit i Hauptspeicherzugriffen herausfinden, also rund tausendmal schneller als bei einer Verkettung der Blöcke im Externspeicher.
Anstatt die Blöcke einer Datei als verkettete Liste zu verwalten, kann man für jede Datei einen Index anlegen. Ein Index ist eine Tabelle, die zu jeder logischen Blocknummer die zugehörige physische Blocknummer enthält. Sie entspricht der Seitentabelle eines Prozesses bei der Hauptspeicherverwaltung; vergleiche Abbildung 2.2. Diese Indextabelle wird selbst auch im Externspei- cher abgelegt.
Auch mit diesem Ansatz ist das Problem der externen Fragmentierung be- seitigt. Heikler ist dagegen die Frage nach der Indexgröße: Bei einer sehr kurzen Datei ist es nicht zu rechtfertigen, einen ganzen Block für die Speicherung des Index zu verwenden. Wenn dagegen die Datei sehr lang ist, reicht ein einzelner Block hierfür nicht aus; es kann dann sogar notwendig werden, einen Index für den Index anzulegen.
In UNIX wird eine recht elegante Variante des Indexprinzips verwendet. Für jede Datei und für jedes Verzeichnis gibt es eine Struktur, die als inode bezeichnet wird; ein Beispiel sehen Sie in Abbildung 2.9.
Am Anfang eines inode stehen die Attribute des Objekts, die wir uns in Abschnitt 2.4.1 mit dem Befehl ls -ls angesehen hatten, wie Zugriffsrechte, Ei- gentümer und Gruppe, Zeitstempel, Größe und Anzahl der Verweise auf das Objekt. Es folgen die physischen Adressen der ersten 12 Blöcke der Datei. Dann kommt die Adresse eines Blocks, der die Adressen der nächsten logischen Dateiblöcke enthält – ein einfach-indirekter Index. Daran schließen die Adres- sen eines zweifach-indirekten und schließlich eines dreifach-indirekten Index- blocks an; dieser enthält die Adressen von zweifach-indirekten Indexblöcken, von denen jeder die Adressen einfach-indirekter Indexblöcke enthält.
Dieses Schema bietet mehrere Vorteile. Zum einen sind alle wesentlichen Informationen über eine Datei oder ein Verzeichnis im inode auf beschränktem Raum zusammengefasst; im Unterschied zur file-allocation table wird aber nur für die wirklich vorhandenen Objekte Speicherplatz belegt. Auf kurze Dateien — oder allgemeiner: auf die ersten 12 Blöcke jeder Datei — kann man sehr effizient zugreifen. Allgemein genügen maximal 5 Zugriffe auf den externen Speicher, um einen beliebigen Block einer langen Datei zu lesen. Wir sehen also: Durch inodes wird wahlfreier Zugriff recht gut unterstützt.
Die FAT bietet eine gute Übersicht darüber, welche Blöcke auf der Fest- platte noch frei sind. Das leisten die inodes nicht. Man kann zu diesem Zweck zusätzlich einen langen Bitvektor verwenden, der für jeden Block ein Bit ent- hält, welches angibt, ob der Block frei oder belegt ist.
Der sequentielle Dateizugriff kann bei allen hier besprochenen Formen der nicht-zusammenhängenden Speicherung mühsam sein — wenn nämlich die Blö- cke über die gesamte Magnetplatte verteilt sind. UNIX versucht, dieses Pro- blem durch eine Anhebung der Blockgröße auf mehrere KByte zu mildern.
Die inodes der vorhandenen Dateien und Verzeichnisse sind in UNIX in einer Tabelle an fester Position auf der Festplatte gespeichert. In den Ver-
zeichnissen (directories) steht beim Namen eines jeden enthaltenen Objekts die Nummer seines inodes in dieser Tabelle; man kann sich die inode-Nummern durch den zusätzlichen Parameter i des ls-Befehls ausgeben lassen. Für jeden Prozess gibt es eine Liste mit den Namen aller Dateien, die dieser Prozess geöffnet hat; diese Liste ist Teil des Prozesskontextes. Es kann durchaus vorkom- men, dass mehrere Prozesse gleichzeitig auf dieselbe Datei zugreifen. Deshalb existiert zusätzlich ein systemweites Verzeichnis aller in Gebrauch befindlicher Dateien, auf das die Einträge in den Prozessverzeichnissen verweisen. In dem globalen Verzeichnis werden ebenfalls die Nummern der inodes verwendet.
